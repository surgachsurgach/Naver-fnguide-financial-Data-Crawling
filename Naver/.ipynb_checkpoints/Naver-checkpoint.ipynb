{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6w-iSBMZ389I"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class Naver:\n",
    "    ##date 형식 맞춰주기\n",
    "    def date_format(self, d=''):\n",
    "        if d != '':\n",
    "            this_date = pd.to_datetime(d).date()\n",
    "        else:\n",
    "            this_date = pd.Timestamp.today().date()   # 오늘 날짜를 지정\n",
    "        return (this_date)\n",
    "    \n",
    "\n",
    "    ##개별종목 주가수집 함수\n",
    "    def stock_price(self, historical_prices, stock_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "\n",
    "        #nvr = self.NaverPrice()\n",
    "        start_date = self.date_format(start_date)\n",
    "        end_date = self.date_format(end_date)\n",
    "\n",
    "        naver_stock = 'http://finance.naver.com/item/sise_day.nhn?code=' + stock_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_stock).read()\n",
    "        source = bs4.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        dates = source.find_all('span', class_='tah p10 gray03')   # 날짜 수집   \n",
    "        prices = source.find_all('td', class_='num')   # 종가 수집\n",
    "        \n",
    "        for n in range(len(dates)):\n",
    "\n",
    "            if len(dates) > 0:\n",
    "\n",
    "                # 날짜 처리\n",
    "                this_date = dates[n].text\n",
    "                this_date = self.date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:   \n",
    "                # start_date와 end_date 사이에서 데이터 저장\n",
    "                    # 종가 처리\n",
    "                    this_close = prices[n*6].text\n",
    "                    this_close = this_close.replace(',', '')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    # 딕셔너리에 저장\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:   \n",
    "                # start_date 이전이면 함수 종료\n",
    "                    return (historical_prices)              \n",
    "\n",
    "        # 페이지 네비게이션\n",
    "        if last_page == 0:\n",
    "            last_page = source.find_all('table')[1].find('td', class_='pgRR').find('a')['href']\n",
    "            last_page = last_page.split('&')[1]\n",
    "            last_page = last_page.split('=')[1]\n",
    "            last_page = float(last_page)\n",
    "\n",
    "        # 다음 페이지 호출\n",
    "        if page_n < last_page:\n",
    "            page_n = page_n + 1\n",
    "            self.stock_price(historical_prices, stock_cd, start_date, end_date, page_n, last_page)   \n",
    "\n",
    "        return (historical_prices)\n",
    "\n",
    "    ##지수 수집\n",
    "    def index_korea(self, historical_prices, index_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
    "    \n",
    "        start_date = self.date_format(start_date)\n",
    "        end_date = self.date_format(end_date)\n",
    "\n",
    "        naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
    "\n",
    "        source = urlopen(naver_index).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = bs4.BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        dates = source.find_all('td', class_='date')   # <td class=\"date\">태그에서 날짜 수집   \n",
    "        prices = source.find_all('td', class_='number_1')   # <td class=\"number_1\">태그에서 지수 수집\n",
    "\n",
    "        for n in range(len(dates)):\n",
    "\n",
    "            if dates[n].text.split('.')[0].isdigit():\n",
    "\n",
    "                # 날짜 처리\n",
    "                this_date = dates[n].text\n",
    "                this_date= self.date_format(this_date)\n",
    "\n",
    "                if this_date <= end_date and this_date >= start_date:   \n",
    "                # start_date와 end_date 사이에서 데이터 저장\n",
    "                    # 종가 처리\n",
    "                    this_close = prices[n*4].text   # prices 중 종가지수인 0,4,8,...번째 데이터 추출\n",
    "                    this_close = this_close.replace(',', '')\n",
    "                    this_close = float(this_close)\n",
    "\n",
    "                    # 딕셔너리에 저장\n",
    "                    historical_prices[this_date] = this_close\n",
    "\n",
    "                elif this_date < start_date:   \n",
    "                # start_date 이전이면 함수 종료\n",
    "                    return (historical_prices)              \n",
    "\n",
    "        # 페이지 네비게이션\n",
    "        if last_page == 0:\n",
    "            last_page = source.find('td', class_='pgRR').find('a')['href']\n",
    "            # 마지막페이지 주소 추출\n",
    "            last_page = last_page.split('&')[1]   # & 뒤의 page=506 부분 추출\n",
    "            last_page = last_page.split('=')[1]   # = 뒤의 페이지번호만 추출\n",
    "            last_page = int(last_page)   # 숫자형 변수로 변환\n",
    "\n",
    "        # 다음 페이지 호출\n",
    "        if page_n < last_page:   \n",
    "            page_n = page_n + 1   \n",
    "            self.index_korea(historical_prices, index_cd, start_date, end_date, page_n, last_page)   \n",
    "\n",
    "        return (historical_prices)  \n",
    "    \n",
    "    ## 구성종목 기본정보\n",
    "    def stock_info(self, stock_cd):\n",
    "        url_float = 'http://companyinfo.stock.naver.com/v1/company/c1010001.aspx?cmp_cd=' + stock_cd\n",
    "        source = urlopen(url_float).read()\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '')\n",
    "        tmp = tmp.replace('\\n', '')\n",
    "        tmp = tmp.replace('\\t', '')\n",
    "\n",
    "        tmp = re.split('/', tmp)\n",
    "\n",
    "        outstanding = tmp[0].replace(',', '')\n",
    "        outstanding = outstanding.replace('주', '')\n",
    "        outstanding = outstanding.replace(' ', '')\n",
    "        outstanding = int(outstanding)\n",
    "\n",
    "        floating = tmp[1].replace(' ', '')\n",
    "        floating = floating.replace('%', '')\n",
    "        floating = float(floating)\n",
    "\n",
    "        name = soup.find(id='pArea').find('div').find('div').find('tr').find('td').find('span').text\n",
    "\n",
    "        #k10_outstanding[stock_cd] = outstanding\n",
    "        #k10_floating[stock_cd] = floating\n",
    "        #k10_name[stock_cd] = name    \n",
    "        \n",
    "        return (name, outstanding, floating)\n",
    "\n",
    "    \n",
    "    \n",
    "    def index_global(self, d, symbol, start_date='', end_date='', page=1):\n",
    "\n",
    "        end_date = self.date_format(end_date)\n",
    "        if start_date == '':\n",
    "            start_date = end_date - pd.DateOffset(years=1)\n",
    "        start_date = self.date_format(start_date)\n",
    "\n",
    "        url = 'https://finance.naver.com/world/worldDayListJson.nhn?symbol='+symbol+'&fdtc=0&page='+str(page)\n",
    "        raw = urlopen(url)\n",
    "        data = json.load(raw)\n",
    "\n",
    "        if len(data) > 0:\n",
    "\n",
    "            for n in range(len(data)):\n",
    "                date = pd.to_datetime(data[n]['xymd']).date()\n",
    "\n",
    "                if date <= end_date and date >= start_date:   \n",
    "                # start_date와 end_date 사이에서 데이터 저장\n",
    "                    # 종가 처리\n",
    "                    price = float(data[n]['clos'])\n",
    "                    # 딕셔너리에 저장\n",
    "                    d[date] = price\n",
    "                elif date < start_date:   \n",
    "                # start_date 이전이면 함수 종료\n",
    "                    return (d)              \n",
    "\n",
    "            if len(data) == 10:\n",
    "                page += 1\n",
    "                self.index_global(d, symbol, start_date, end_date, page)\n",
    "\n",
    "        return (d)\n",
    "    \n",
    "    \n",
    "class NaverStockInfo:\n",
    "    ##기업정보\n",
    "    def read_src(self, stock_cd):\n",
    "        url_float = 'http://companyinfo.stock.naver.com/v1/company/c1010001.aspx?cmp_cd=' + stock_cd\n",
    "        source = urlopen(url_float).read()\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "        return (soup)\n",
    "        \n",
    "    \n",
    "    def stock_info(self, stock_cd):\n",
    "        url_float = 'http://companyinfo.stock.naver.com/v1/company/c1010001.aspx?cmp_cd=' + stock_cd\n",
    "        source = urlopen(url_float).read()\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '')\n",
    "        tmp = tmp.replace('\\n', '')\n",
    "        tmp = tmp.replace('\\t', '')\n",
    "\n",
    "        tmp = re.split('/', tmp)\n",
    "\n",
    "        outstanding = tmp[0].replace(',', '')\n",
    "        outstanding = outstanding.replace('주', '')\n",
    "        outstanding = outstanding.replace(' ', '')\n",
    "        outstanding = int(outstanding)\n",
    "\n",
    "        floating = tmp[1].replace(' ', '')\n",
    "        floating = floating.replace('%', '')\n",
    "        floating = float(floating)\n",
    "\n",
    "        name = soup.find(id='pArea').find('div').find('div').find('tr').find('td').find('span').text\n",
    "       \n",
    "        return (name, outstanding, floating)\n",
    "      \n",
    "    ##발행주식수\n",
    "    def outstanding(self, stock_cd):\n",
    "        soup = self.read_src(stock_cd)\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '')\n",
    "        tmp = tmp.replace('\\n', '')\n",
    "        tmp = tmp.replace('\\t', '')\n",
    "        tmp = re.split('/', tmp)\n",
    "        outstanding = tmp[0].replace(',', '')\n",
    "        outstanding = outstanding.replace('주', '')\n",
    "        outstanding = outstanding.replace(' ', '')\n",
    "        outstanding = int(outstanding)\n",
    "        return (outstanding)\n",
    "      \n",
    "    ##유동주식수\n",
    "    def floating(self, stock_cd):\n",
    "        soup = self.read_src(stock_cd)\n",
    "        tmp = soup.find(id='cTB11').find_all('tr')[6].td.text\n",
    "        tmp = tmp.replace('\\r', '')\n",
    "        tmp = tmp.replace('\\n', '')\n",
    "        tmp = tmp.replace('\\t', '')\n",
    "        tmp = re.split('/', tmp)\n",
    "        floating = tmp[1].replace(' ', '')\n",
    "        floating = floating.replace('%', '')\n",
    "        floating = float(floating)\n",
    "        return (floating)\n",
    "    \n",
    "    ##formatting\n",
    "    def float_convert(self, s):\n",
    "        try:\n",
    "            s = s.replace(' ', '')\n",
    "            s = s.replace(',', '')\n",
    "            if re.findall('억', s):\n",
    "                m = 100000000\n",
    "                s = s.replace('억', '')\n",
    "            elif re.findall('백만', s):\n",
    "                m = 1000000\n",
    "                s = s.replace('백만', '')\n",
    "            if re.findall('%', s):\n",
    "                m = 0.01\n",
    "                s = s.replace('%', '')\n",
    "            s = s.replace('원', '')\n",
    "            f = float(s) * m\n",
    "        except:\n",
    "            f = s\n",
    "        return (f)\n",
    "    \n",
    "    ##fundamental\n",
    "    def fundamentals(self, stock_cd, f):\n",
    "        factors = dict()\n",
    "        soup = self.read_src(stock_cd)\n",
    "        rows = len(soup.find_all('div', class_='fund fl_le')[0].find_all('tr'))\n",
    "        for r in range(1, rows, 1):\n",
    "            title = soup.find_all('div', class_='fund fl_le')[0].find_all('tr')[r].find_all('th')[0].text\n",
    "            value_current = soup.find_all('div', class_='fund fl_le')[0].find_all('tr')[r].find_all('td')[0].text\n",
    "            value_current = self.float_convert(value_current)\n",
    "            value_estimated = soup.find_all('div', class_='fund fl_le')[0].find_all('tr')[r].find_all('td')[1].text\n",
    "            value_estimated = self.float_convert(value_estimated)\n",
    "            factors[title] = [value_current, value_estimated]\n",
    "            print(title, value_current, value_estimated)\n",
    "        return (factors[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObCmm4Ty4Vbn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "class NaverCrawler:\n",
    "    \n",
    "    def __init__(self, keywords, page_cnt):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument('window-size=1920x1080')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "        self.driver = webdriver.Chrome(chrome_options=options)\n",
    "        self.keywords = keywords\n",
    "        self.hos2url={}\n",
    "        self.url2hos={}\n",
    "        self.power_link_df=None\n",
    "        self.place_df=None\n",
    "        self.errIdx=[]\n",
    "        self.page_cnt=page_cnt\n",
    "    \n",
    "    def crawl(self):\n",
    "        url = 'https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query={}'\n",
    "        for idx in range(len(self.keywords)):\n",
    "            self.driver.get(url.format(self.keywords[idx]))\n",
    "            self.driver.implicitly_wait(random.randrange(2,4))\n",
    "            self.scroll()\n",
    "            try:\n",
    "                self.power_link_crawl()\n",
    "            except Exception as e:\n",
    "                print(idx,'no power link : ',e)\n",
    "                self.errIdx.append(('power',idx))\n",
    "                \n",
    "            try:\n",
    "                self.place_crawl()\n",
    "            except Exception as e:\n",
    "                print(idx,'no place: ',e)\n",
    "                self.errIdx.append(('place',idx))\n",
    "    \n",
    "    def power_link_crawl(self):\n",
    "        title_list = []\n",
    "        url_list = []\n",
    "        text_list = []\n",
    "        score_list = []\n",
    "        link_titles=self.driver.find_element_by_id('power_link_body').find_elements_by_class_name('lnk_tit')\n",
    "        link_urls=self.driver.find_element_by_id('power_link_body').find_elements_by_class_name('lnk_url')\n",
    "        link_texts=self.driver.find_element_by_id('power_link_body').find_elements_by_class_name('ad_dsc_inner')\n",
    "        score=1\n",
    "        \n",
    "        for title,url,text in zip(link_titles,link_urls,link_texts):\n",
    "            title_list.append(title.text)\n",
    "            url_list.append(url.text)\n",
    "            text_list.append(text.text)\n",
    "            score_list.append(score)\n",
    "            score=score-1/15\n",
    "        tmp_df=pd.DataFrame({'power_title':title_list,'power_url_list':url_list,'power_text':text_list,'power_score':score_list})\n",
    "        if type(self.power_link_df)==type(None):\n",
    "            self.power_link_df = tmp_df\n",
    "        else:\n",
    "            self.power_link_df = self.power_link_df.append(tmp_df, ignore_index = True)\n",
    "            \n",
    "    def place_crawl(self):\n",
    "        place_title_list = []\n",
    "        place_url_list = []\n",
    "        place_score_list=[]\n",
    "\n",
    "        for idx in range(self.page_cnt):\n",
    "            score = 1-idx*0.1\n",
    "            tmp_element_list = self.driver.find_elements_by_xpath(\"*//div[@class='list_area']//a[@class='name']\")\n",
    "            place_score_list.extend([score]*len(tmp_element_list))\n",
    "            for element in tmp_element_list:\n",
    "                place_title_list.append(element.text.split(' ')[1])\n",
    "                place_url_list.append(element.get_attribute('href'))\n",
    "            self.click_next_btn()\n",
    "            time.sleep(random.random()/2) \n",
    "        tmp_df = pd.DataFrame({'place_title':place_title_list,'place_url':place_url_list,'place_score':place_score_list})\n",
    "        tmp_df.head()\n",
    "        if type(self.place_df) == type(None):\n",
    "            self.place_df = tmp_df\n",
    "        else:\n",
    "            self.place_df = self.place_df.append(tmp_df, ignore_index = True)\n",
    "    \n",
    "    def scroll(self):\n",
    "        for i in range(1,11):\n",
    "            self.driver.execute_script(\"window.scrollTo(document.body.scrollHeight/10*\"+str(i-1)+\", document.body.scrollHeight/11*\"+str(i)+\");\")\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    def click_next_btn(self):\n",
    "        self.driver.find_element_by_xpath(\"*//a[@class='btn_direction btn_next ']\").click()\n",
    "    \n",
    "    def get_power_link_df(self):\n",
    "        return self.power_link_df\n",
    "    \n",
    "    def get_place_df(self):\n",
    "        return self.place_df\n",
    "    \n",
    "    def save_power_link_as_csv(self):\n",
    "        pre_file_name = '{}-{}-{}-'.format(datetime.now().year,datetime.now().month,datetime.now().day)\n",
    "        self.power_link_df.to_csv(pre_file_name+'power.csv', encoding='euc-kr')\n",
    "        \n",
    "    def save_place_as_csv(self):\n",
    "        pre_file_name = '{}-{}-{}-'.format(datetime.now().year,datetime.now().month,datetime.now().day)\n",
    "        self.place_df.to_csv(pre_file_name+'place.csv', encoding='euc-kr')\n",
    "if __name__ == \"__main__\":\n",
    "    pre_file_name = '{}-{}-{}-'.format(datetime.now().year,datetime.now().month,datetime.now().day)\n",
    "    df = pd.read_csv(pre_file_name+'연관검색어.csv', encoding='euc-kr',index_col=0)\n",
    "    nc = NaverCrawler(df['키워드'],5)\n",
    "    nc.crawl()\n",
    "    nc.get_place_df().to_csv(pre_file_name+'place.csv', encoding='euc-kr')\n",
    "    nc.get_power_link_df().to_csv(pre_file_name+'power.csv', encoding='euc-kr')\n",
    "    pd.Series(nc.errIdx).to_csv('err_log.csv')      "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Naver.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
